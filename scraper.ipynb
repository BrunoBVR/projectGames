{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping games info from metacritic\n",
    "\n",
    "We will use [Metacritic](https://www.metacritic.com/browse/games/score/metascore/all/all/filtered?sort=desc) data to create a dataframe with data on games across all platforms and all time.\n",
    "\n",
    "## The dataset\n",
    "\n",
    "We want to get a dataframe with all games with columns:\n",
    "* **name**: The name of the game\n",
    "* **platform**: Platform it was released\n",
    "* **r-date**: date it was released\n",
    "* **score**: average score given by critics (metascore)\n",
    "* **user score**: average score given by users in the website\n",
    "* **developer**: game developer\n",
    "* **genre**: genre of the game (can be multiple)\n",
    "* **players**: Number of players (some games don't have this information)\n",
    "* **critics**: number of critics reviewing the game\n",
    "* **users**: Number of metacritic users that reviewed the game\n",
    "\n",
    "**All data was collected on November 10th, 2020.**\n",
    "\n",
    "## Steps used in scraper:\n",
    "\n",
    "* Create a dictionary `pages` that will contain the DataFrame objects from all pages. Each entry is a pandas DataFrame with data from the games in each site page. There are, currently, 180 pages of rated games.\n",
    "* For each page, create a dictionary `data_page` of empty lists to be filled with the data from each game. As each page displays 100 games, each of this lists should contain 100 elements (except for the last page).\n",
    "* Use `requests` to get into the url of each page and `BEautifulSoup` to parse the html file.\n",
    "* Loop through all games in each page and scrap the relevant data. Note that 'developer', 'genre', 'players', 'critics' and 'users' are found on different URLs, so we need to fetch these for each game. This URL for each game is inside a `a` tag with a `title` class. \n",
    "* There are a couple of if's in the scraper to ensure None objects get dealt with (some games don't have a number of players information, for example; some games have no user reviews, given it is not yet released, and a few others).\n",
    "* After all data is collected (and it **takes a few hours** - a bit more than 15 in my laptop), all dataframes in the `pages` dictionary are concatenated to create a single one with all game data.\n",
    "* The dataframe is export to a csv file.\n",
    "* I enjoy the awesome new dataset and all I can do with it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = {}\n",
    "\n",
    "for page in range(180):\n",
    "    \n",
    "    data_page = {\n",
    "        'name':[],\n",
    "        'platform':[],\n",
    "        'r-date':[],\n",
    "        'score':[],\n",
    "        'user score':[],\n",
    "        'developer':[],\n",
    "        'genre':[],\n",
    "        'players':[],\n",
    "        'critics':[],\n",
    "        'users':[]\n",
    "    }    \n",
    "    \n",
    "    # Site inside metacritic listing \"Game Releases by Score\"\n",
    "    url = 'https://www.metacritic.com/browse/games/score/metascore/all/all/filtered?page='+str(page)\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Printing out current page\n",
    "    print(50*'=', \"In page: \", page)\n",
    "    \n",
    "    # Loop through all games in current page\n",
    "    for game in soup.find_all('td', class_ = 'clamp-summary-wrap'):\n",
    "        # Name\n",
    "        data_page['name'].append(game.find('h3').text)\n",
    "        \n",
    "        print(game.find('h3').text, end=\"\\r\")\n",
    "        \n",
    "        # Platform\n",
    "        platform = game.find('span', class_='data').text\n",
    "\n",
    "        # Removing white space\n",
    "        platform = platform.replace('\\n','')\n",
    "        platform = platform.replace(' ','')\n",
    "\n",
    "        data_page['platform'].append(platform)\n",
    "        \n",
    "        # Release date\n",
    "        data_page['r-date'].append(game.select('div.clamp-details span')[2].text)\n",
    "        \n",
    "        # MetaScore (has different classes depending on score)\n",
    "        score_list = [\n",
    "            game.find('div', class_='metascore_w large game positive'),\n",
    "            game.find('div', class_='metascore_w large game mixed'),\n",
    "            game.find('div', class_='metascore_w large game negative')\n",
    "        ]\n",
    "        \n",
    "        # Filtering not none element in the score_list\n",
    "        score = [s.text for s in score_list if s is not None][0]\n",
    "        \n",
    "        data_page['score'].append(score)\n",
    "        \n",
    "        # User Score (has different classes depending on score)\n",
    "        score_list = [\n",
    "            game.find('div', class_='metascore_w user large game positive'),\n",
    "            game.find('div', class_='metascore_w user large game mixed'),\n",
    "            game.find('div', class_='metascore_w user large game negative'),\n",
    "            game.find('div', class_='metascore_w user large game tbd')\n",
    "        ]\n",
    "        \n",
    "        # Filtering not none element in the score_list\n",
    "        score = [s.text for s in score_list if s is not None][0]\n",
    "        \n",
    "        data_page['user score'].append(score)\n",
    "        \n",
    "        # Into the game page\n",
    "        # Getting the url of the reviews page:\n",
    "        url_info = game.find('a', class_='title')['href']\n",
    "\n",
    "        url_info = 'https://www.metacritic.com'+url_info\n",
    "\n",
    "        # Getting into the game page:\n",
    "        response_info = requests.get(url_info, headers = user_agent)\n",
    "\n",
    "        soup_info = BeautifulSoup(response_info.text, 'html.parser')\n",
    "\n",
    "        # Get developer info\n",
    "\n",
    "        developer = soup_info.find('li', class_ = 'summary_detail developer')\n",
    "        \n",
    "        if developer is not None:\n",
    "            developer = developer.find('span',class_='data').text\n",
    "\n",
    "            developer = developer.replace('\\n','')\n",
    "            developer = developer.replace(' ','')  \n",
    "\n",
    "            data_page['developer'].append(developer)\n",
    "        else:\n",
    "            data_page['developer'].append('No info')\n",
    "\n",
    "        # Get genre info (multiple genres are separated by commas in our entry)\n",
    "\n",
    "        genres = soup_info.find('li', class_ = 'summary_detail product_genre')\n",
    "        \n",
    "        if genres is not None:\n",
    "            genres = genres.find_all('span', class_='data')\n",
    "            genre=''\n",
    "\n",
    "            for item in genres:\n",
    "                if genre:\n",
    "                    genre = genre + ',' + item.text\n",
    "                else:\n",
    "                    genre = item.text\n",
    "\n",
    "            data_page['genre'].append(genre)\n",
    "        else:\n",
    "            data_page['genre'].append('No info')\n",
    "\n",
    "        # Get number of players\n",
    "\n",
    "        players = soup_info.find('li', class_ = 'summary_detail product_players')\n",
    "        \n",
    "        if players is not None:\n",
    "            players = players.find('span',class_='data').text\n",
    "            data_page['players'].append(players)\n",
    "        else:\n",
    "            data_page['players'].append('No info')\n",
    "\n",
    "        # Get number of critics\n",
    "\n",
    "        critics = soup_info.find('div',class_='score_summary metascore_summary')\n",
    "        \n",
    "        if critics is not None:\n",
    "            critics = critics.find('div',class_='summary').find('a').find('span').text\n",
    "\n",
    "            if critics is not None:\n",
    "\n",
    "                critics = critics.replace('\\n','')\n",
    "                critics = critics.replace(' ','')  \n",
    "\n",
    "                data_page['critics'].append(critics)\n",
    "\n",
    "            else:\n",
    "                data_page['critics'].append('0')\n",
    "        else:\n",
    "            data_page['critics'].append('0')\n",
    "\n",
    "        # get number of users\n",
    "\n",
    "        users = soup_info.find('div',class_='details side_details')\n",
    "        \n",
    "        if users is not None:\n",
    "            users = users.find('div',class_='score_summary')\n",
    "\n",
    "            if users is not None:\n",
    "                users = users.find('span',class_='count').find('a')\n",
    "\n",
    "                if users is not None:\n",
    "                    users = users.text\n",
    "                    users = re.sub('\\ Ratings$', '', users)\n",
    "                    data_page['users'].append(users)\n",
    "                else:\n",
    "                    data_page['users'].append('0')\n",
    "            else:\n",
    "                data_page['users'].append('0')\n",
    "        else:\n",
    "            data_page['users'].append('0')\n",
    "            \n",
    "            \n",
    "    # create a dict entry to store the dataframe for each page\n",
    "    pages[str(page)] = pd.DataFrame(data_page)\n",
    "    \n",
    "    # export page data as csv\n",
    "    pages[str(page)].to_csv('games_data-page'+str(page)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating all dataframes inside 'pages' dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all dataframes to concatenate\n",
    "frames = []\n",
    "\n",
    "for k,v in pages.items():\n",
    "    frames.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reseting the indexes (not really necessary...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate.index = range(len(df_ultimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate.to_csv('games-data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
